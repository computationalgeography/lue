from .strong_scaling_experiment import StrongScalingExperiment
from .. import dataset
from .. import job
from .. import util

import lue
import dateutil.relativedelta
import dateutil.parser
import numpy as np
from tqdm import trange
import csv
from functools import reduce  # Python 3
import json
import os.path
import tempfile


def benchmark_meta_to_lue_json(
        benchmark_pathname,
        lue_dataset_pathname,
        cluster,
        benchmark,
        experiment):

    array_shape = experiment.array.shape
    partition_shape = experiment.partition.shape

    lue_json = {
        "dataset": {
            "phenomena": [
                {
                    "name": "benchmark",
                    "collection_property_sets": [
                        {
                            "name": "meta_information",
                            "properties": [
                                {
                                    "name": "name",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "string",
                                    "value": [experiment.program_name]
                                },
                                {
                                    "name": "system_name",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "string",
                                    "value": [cluster.name]
                                },
                                {
                                    "name": "command",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "string",
                                    "value": [experiment.command_pathname]
                                },
                                {
                                    "name": "kind",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "string",
                                    "value": [experiment.name]
                                },
                                {
                                    "name": "scenario_name",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "string",
                                    "value": [benchmark.scenario_name]
                                },
                                {
                                    "name": "description",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "string",
                                    "value": [experiment.description]
                                },
                                {
                                    "name": "nr_time_steps",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "uint64",
                                    "value": [experiment.nr_time_steps]
                                },
                                {
                                    "name": "array_shape",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "uint64",
                                    "shape": [len(array_shape)],
                                    "value": array_shape
                                },
                                {
                                    "name": "partition_shape",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "uint64",
                                    "shape": [len(partition_shape)],
                                    "value": partition_shape
                                },
                                {
                                    "name": "worker_type",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "constant",
                                    "datatype": "string",
                                    "value": [benchmark.worker.type]
                                },
                            ]
                        }
                    ]
                }
            ]
        }
    }

    # Write results
    open(lue_dataset_pathname, "w").write(
        json.dumps(lue_json, sort_keys=False, indent=4))


def benchmark_to_lue_json(
        benchmark_pathname,
        lue_json_pathname,
        epoch):

    # Read benchmark JSON
    benchmark_json = json.loads(open(benchmark_pathname).read())

    time_units = benchmark_json["unit"]
    benchmark_epoch = dateutil.parser.isoparse(benchmark_json["start"])

    # We assume here that benchmarks are located at different time points
    # in seconds from each other. If this is not the case, use time_units
    # to figure out what units to use instead.
    epoch_offset = int((benchmark_epoch - epoch).total_seconds())

    if epoch_offset < 0:
        raise RuntimeError(
            "epoch passed in is later than epoch from benchmark: "
            "{} > {}".format(epoch, benchmark_epoch))

    # Calculate number of seconds sinds the epoch
    time_points = [
        dateutil.parser.isoparse(timing["start"])
            for timing in benchmark_json["timings"]]
    time_points = [
        epoch_offset + int((time_point - benchmark_epoch).total_seconds())
            for time_point in time_points]

    # Just pick the first one for these count benchmarks
    time_points = [time_points[0]]

    property_description = "Amount of time a measurement took"
    durations = [timing["duration"] for timing in benchmark_json["timings"]]

    # Object tracking: a benchmark contains property values (durations)
    # for a single object (piece of software being benchmarked). The ID of
    # this object is some value, like 5.
    # The active set indices are 0, 1, 2, 3, ...
    nr_active_sets = len(time_points)
    active_set_idx = list(range(nr_active_sets))
    active_object_id = nr_active_sets * [5]

    nr_workers = benchmark_json["environment"]["nr_workers"]

    lue_json = {
        "dataset": {
            "phenomena": [
                {
                    "name": "benchmark",
                    "property_sets": [
                        {
                            "name": "measurement",
                            "description":
                                "Information per benchmark measurement",
                            "object_tracker": {
                                "active_set_index": active_set_idx,
                                "active_object_id": active_object_id
                            },
                            "time_domain": {
                                "clock": {
                                    "epoch": {
                                        "kind": "common_era",
                                        "origin": epoch.isoformat(),
                                        "calendar": "gregorian"
                                    },
                                    "unit": time_units,
                                    "tick_period_count": 1
                                },
                                "time_point": time_points
                            },
                            "properties": [
                                {
                                    "name": "duration",
                                    "description": property_description,
                                    "shape_per_object": "same_shape",
                                    "value_variability": "variable",
                                    "shape_variability": "constant_shape",
                                    "datatype": "uint64",
                                    "shape": [len(durations)],
                                    "value": durations
                                },
                                {
                                    "name": "nr_workers",
                                    "shape_per_object": "same_shape",
                                    "value_variability": "variable",
                                    "shape_variability": "constant_shape",
                                    "datatype": "uint64",
                                    "value": [nr_workers]
                                },
                            ]
                        }
                    ]
                }
            ]
        }
    }

    # Write results
    open(lue_json_pathname, "w").write(
        json.dumps(lue_json, sort_keys=False, indent=4))


def import_raw_results(
        lue_dataset_pathname,
        cluster,
        benchmark,
        experiment):
    """
    Import all raw benchmark results into a new LUE file

    This is a two step process:
    1. Translate each raw benchmark result into a LUE JSON file
    2. Import all LUE JSON files into a single LUE file
    """
    # Each benchmark containing timings has a start location in time and
    # an overall duration. The location in time can be used to position
    # the benchmark in time. Most likely, all benchmarks are started at
    # different locations in time. The duration is not that relevant.

    # The timings are represented by a location in time and a
    # duration. The location in time is not that relevant. Duration is.

    # To position all benchmarks in time, we need a single starting time
    # point to use as the clock's epoch and calculate the distance of
    # each benchmark's start time point from this epoch.

    # Create a collection of benchmark indices where the indices are
    # sorted by start location in time

    # It is possible that the results for a later experiment, with more
    # workers, is stored before an experiment with less workers. This
    # must be taken care of later, during post-processing.
    # -> Results are sorted by time, not by the number of workers!!!

    benchmark_idxs, epoch = util.sort_benchmarks_by_time(
        cluster, benchmark, experiment)

    metadata_written = False

    for benchmark_idx in benchmark_idxs:

        nr_workers = benchmark.worker.nr_workers(benchmark_idx)

        result_pathname = experiment.benchmark_result_pathname(
            cluster.name, benchmark.scenario_name, nr_workers, "json")
        assert os.path.exists(result_pathname), result_pathname

        if not metadata_written:
            with tempfile.NamedTemporaryFile(suffix=".json") as lue_json_file:
                benchmark_meta_to_lue_json(
                    result_pathname, lue_json_file.name,
                    cluster, benchmark, experiment)
                util.import_lue_json(lue_json_file.name, lue_dataset_pathname)
            metadata_written = True

        with tempfile.NamedTemporaryFile(suffix=".json") as lue_json_file:
            benchmark_to_lue_json(
                result_pathname, lue_json_file.name, epoch)
            util.import_lue_json(lue_json_file.name, lue_dataset_pathname)

    lue.assert_is_valid(lue_dataset_pathname)


def write_scaling_results(
        lue_dataset):

    count = lue_dataset.benchmark.measurement.duration.value.shape[1]

    lue_measurement = lue_dataset.benchmark.measurement
    lue_meta_information = lue_dataset.benchmark.meta_information

    # Write property-set to dataset containing the scalability information
    # - benchmark
    #     - scaling
    #         - (mean_duration)
    #         - (std_duration)
    #         - relative speed-up (+ mean, std)
    #         - efficiency (+ mean, std)
    #         - LUPS (+ mean, std)
    scaling_property_set = lue_dataset.benchmark.add_property_set(
        "scaling", lue_measurement.time_domain, lue_measurement.object_tracker)

    duration = lue_measurement.duration.value[:]
    nr_durations = len(duration)

    nr_workers = lue_measurement.nr_workers.value[:]

    # Results are sorted by time, not by nr_workers. Find index of
    # benchmark with 1 worker.
    t1_idx = np.where(nr_workers == 1)[0][0]
    assert nr_workers[t1_idx] == 1, nr_workers

    nr_workers = nr_workers.reshape(len(nr_workers), 1)

    # Count durations, using one worker
    t1 = duration[t1_idx].astype(np.float64)

    # speed_up = t1 / tn
    relative_speed_up = t1 / duration

    relative_speed_up_property = scaling_property_set.add_property(
        "relative_speed_up", np.dtype(np.float64), shape=(count,),
        value_variability=lue.ValueVariability.variable,
        description="Relative speed-up: t1 / duration")
    relative_speed_up_property.value.expand(nr_durations)[:] = \
        relative_speed_up

    # efficiency = 100% * speed_up / nr_workers
    relative_efficiency = 100 * relative_speed_up / nr_workers

    relative_efficiency_property = scaling_property_set.add_property(
        "relative_efficiency", np.dtype(np.float64), shape=(count,),
        value_variability=lue.ValueVariability.variable,
        description="Relative efficiency: 100% * relative_speed_up / "
        "nr_workers")
    relative_efficiency_property.value.expand(nr_durations)[:] = \
        relative_efficiency

    # lups = nr_time_steps * nr_elements / duration
    # In the case of strong scaling, the nr_elements is
    # constant. Ideally, LUPS increases linearly with the nr_workers.
    nr_time_steps = lue_meta_information.nr_time_steps.value[0]
    array_shape = lue_meta_information.array_shape.value[0]
    nr_elements = reduce(lambda e1, e2: e1 * e2, array_shape)
    lups = nr_time_steps * nr_elements / duration

    lups_property = scaling_property_set.add_property(
        "lups", np.dtype(np.float64), shape=(count,),
        value_variability=lue.ValueVariability.variable,
        description="LUPS: nr_time_steps * nr_elements / "
        "duration")
    lups_property.value.expand(nr_durations)[:] = lups

    if count > 1:
        mean_duration = np.mean(duration, axis=1)
        std_duration = np.std(duration, axis=1)
        assert len(mean_duration) == len(std_duration) == nr_durations

        mean_relative_speed_up = np.mean(relative_speed_up, axis=1)
        std_relative_speed_up = np.std(relative_speed_up, axis=1)

        mean_relative_efficiency = np.mean(relative_efficiency, axis=1)
        std_relative_efficiency = np.std(relative_efficiency, axis=1)

        mean_lups = np.mean(lups, axis=1)
        std_lups = np.std(lups, axis=1)

        mean_duration_property = scaling_property_set.add_property(
            "mean_duration", np.dtype(np.float64), shape=(),
            value_variability=lue.ValueVariability.variable,
            description=
                "For a number of workers, the mean duration of the {} "
                "experiments took."
                    .format(count))
        mean_duration_property.value.expand(nr_durations)[:] = mean_duration

        std_duration_property = scaling_property_set.add_property(
            "std_duration", np.dtype(np.float64), shape=(),
            value_variability=lue.ValueVariability.variable,
            description=
                "For a number of workers, the standard deviation of the "
                "durations the {} experiments took."
                    .format(count))
        std_duration_property.value.expand(nr_durations)[:] = std_duration

        mean_relative_speed_up_property = scaling_property_set.add_property(
            "mean_relative_speed_up", np.dtype(np.float64), shape=(),
            value_variability=lue.ValueVariability.variable,
            description=
                "For a number of workers, the mean of the relative "
                "speed-up of the {} experiments."
                    .format(count))
        mean_relative_speed_up_property.value.expand(nr_durations)[:] = \
            mean_relative_speed_up

        std_relative_speed_up_property = scaling_property_set.add_property(
            "std_relative_speed_up", np.dtype(np.float64), shape=(),
            value_variability=lue.ValueVariability.variable,
            description=
                "For a number of workers, the standard deviation of the "
                "relative speed-ups of the {} experiments."
                    .format(count))
        std_relative_speed_up_property.value.expand(nr_durations)[:] = \
            std_relative_speed_up

        mean_relative_efficiency_property = scaling_property_set.add_property(
            "mean_relative_efficiency", np.dtype(np.float64), shape=(),
            value_variability=lue.ValueVariability.variable,
            description=
                "For a number of workers, the mean of the relative "
                "efficiency of the {} experiments."
                    .format(count))
        mean_relative_efficiency_property.value.expand(nr_durations)[:] = \
            mean_relative_efficiency

        std_relative_efficiency_property = scaling_property_set.add_property(
            "std_relative_efficiency", np.dtype(np.float64), shape=(),
            value_variability=lue.ValueVariability.variable,
            description=
                "For a number of workers, the standard deviation of the "
                "relative efficiency of the {} experiments."
                    .format(count))
        std_relative_efficiency_property.value.expand(nr_durations)[:] = \
            std_relative_efficiency

        mean_lups_property = scaling_property_set.add_property(
            "mean_lups", np.dtype(np.float64), shape=(),
            value_variability=lue.ValueVariability.variable,
            description=
                "For a number of workers, the mean of the LUPS of the {} "
                "experiments."
                    .format(count))
        mean_lups_property.value.expand(nr_durations)[:] = mean_lups

        std_lups_property = scaling_property_set.add_property(
            "std_lups", np.dtype(np.float64), shape=(),
            value_variability=lue.ValueVariability.variable,
            description=
                "For a number of workers, the standard deviation of the "
                "LUPS of the {} experiments."
                    .format(count))
        std_lups_property.value.expand(nr_durations)[:] = std_lups

    lue.assert_is_valid(lue_dataset)


def read_performance_counters(
        counter_pathname):

    with open(counter_pathname, "r") as csv_file:
        reader = csv.reader(csv_file)
        field_names = next(reader)

    # HPX appends to CSVs... If this fails, remove CSVs and run experiment
    # again. Or remove earlier stuff from CSVs.
    array = np.loadtxt(counter_pathname, dtype=np.float64, unpack=True,
        delimiter=',', skiprows=1)

    if len(field_names) == 1:
        array = array.reshape(1, len(array))

    assert len(array) == len(field_names), "{}: {} != {}".format(
        counter_pathname, len(array), len(field_names))

    # # We are assuming uint64 values here
    # assert np.all(array >= 0)
    # assert np.all(array % 1 == 0)
    # array = array.astype(np.uint64)

    # Idle-rates are reported as 0.01%. Convert them to percentages.
    for i in range(len(field_names)):
        if "idle-rate" in field_names[i]:
            array[i] /= 100.

    return field_names, array


def import_performance_counter_file(
        lue_dataset,
        hpx,
        nr_workers,
        counter_pathname):

    # --------------------------------------------------------------------------
    # Read CSV into list of field names and 2D array of arrays with
    # counter values, shaped (nr_time_steps, nr_fields)
    with open(counter_pathname, "r") as csv_file:
        reader = csv.reader(csv_file)
        field_names = next(reader)

    # HPX appends to CSVs... If this fails, remove CSVs and run experiment
    # again. Or remove earlier stuff from CSVs.
    counter_values = np.loadtxt(
        counter_pathname, dtype=np.float64, delimiter=',', skiprows=1, ndmin=1)
    assert len(counter_values.shape) == 2
    nr_time_steps = counter_values.shape[0]

    # ndmin should render this unnecessary
    # if len(field_names) == 1:
    #     counter_values = counter_values.reshape(1, len(counter_values))

    # assert len(counter_values) == len(field_names), "{}: {} != {}".format(
    #     counter_pathname, len(counter_values), len(field_names))
    assert counter_values.shape[1] == len(field_names), "{}: {} != {}".format(
        counter_pathname, counter_values.shape[1], len(field_names))

    # # We are assuming uint64 values here
    # assert np.all(counter_values >= 0)
    # assert np.all(counter_values % 1 == 0)
    # counter_values = counter_values.astype(np.uint64)

    # Idle-rates are reported as 0.01%. Convert them to percentages.
    for i in range(len(field_names)):
        if "idle-rate" in field_names[i]:
            counter_values[:, i] /= 100.

    # --------------------------------------------------------------------------
    # Create property-set for storing the counter values. Each performance
    # counter becomes a property in the set. The time domain is a time
    # box which is discretized in time steps. Each time step has the
    # size of the resolution used while measuring the performance counters.

    lue_benchmark = lue_dataset.benchmark

    time_configuration = lue.TimeConfiguration(
        lue.TimeDomainItemType.box
    )
    clock = lue.Clock(lue.Unit.millisecond, hpx.counter_interval)

    lue_performance_counter = lue_benchmark.add_property_set(
        "performance_counter_{}".format(nr_workers),
        time_configuration, clock)

    # The one active object
    lue_performance_counter.object_tracker.active_set_index.expand(1)[:] = \
        np.array([0])
    lue_performance_counter.object_tracker.active_object_id.expand(1)[:] = \
        np.array([5])

    # The one time box
    lue_performance_counter.time_domain.value.expand(1)[:] = \
        np.array([0, nr_time_steps * hpx.counter_interval]).reshape(1, 2)

    # Add properties to property-set
    for i in range(len(field_names)):
        property_name = \
            util.performance_counter_name_to_property_name(field_names[i])
        value_property = lue_performance_counter.add_property(
            property_name, dtype=np.dtype(np.float64),
            rank=1, shape_per_object=lue.ShapePerObject.different,
            shape_variability=lue.ShapeVariability.variable)

        # Skip for now... We know what it is.
        # discretization_property = 

        # For the one and only timebox (idx == 0) and this benchmark (ID == 5)
        value_property.value.expand(0, 5, (nr_time_steps,))[5][:] = counter_values[:, i]


def import_performance_counters(
        lue_dataset,
        hpx,
        experiment):

    # Iterate over all files containing performance counter information
    # and import data into the properties of a new property-set

    lue_benchmark = lue_dataset.benchmark
    lue_meta_information = \
        lue_benchmark.collection_property_sets["meta_information"]
    system_name = lue_meta_information.properties["system_name"].value[:][0]
    scenario_name = lue_meta_information.properties["scenario_name"].value[:][0]

    lue_measurement = lue_benchmark.property_sets["measurement"]
    nr_workers = lue_measurement.properties["nr_workers"].value[:]

    for i in trange(len(nr_workers), desc="import performance counters"):
        counter_pathname = experiment.benchmark_result_pathname(
            system_name, scenario_name, "counter-{}".format(nr_workers[i]), "csv")
        assert os.path.exists(counter_pathname), counter_pathname

        import_performance_counter_file(
            lue_dataset, hpx, nr_workers[i], counter_pathname)

    lue.assert_is_valid(lue_dataset, fail_on_warning=False)


def import_results(
        results_prefix):

    lue_dataset = job.open_raw_lue_dataset(results_prefix, "r")
    raw_results_already_imported = \
        dataset.raw_results_already_imported(lue_dataset)

    if not raw_results_already_imported:
        cluster, benchmark, experiment = dataset.read_benchmark_settings(
            lue_dataset, StrongScalingExperiment)
        import_raw_results(
            lue_dataset.pathname, cluster, benchmark, experiment)

    if not raw_results_already_imported or \
            not job.scaling_lue_dataset_exists(results_prefix):

        # Copy dataset and write scaling results
        job.copy_raw_to_scaling_lue_dataset(results_prefix)
        lue_dataset = job.open_scaling_lue_dataset(results_prefix, "w")
        write_scaling_results(lue_dataset)

        performance_counters_available = benchmark.hpx is not None and \
            benchmark.hpx.performance_counters is not None

        if performance_counters_available:
            import_performance_counters(lue_dataset, benchmark.hpx, experiment)

        lue_dataset.flush()  # TODO Why is this needed?
        lue.assert_is_valid(lue_dataset, fail_on_warning=False)
